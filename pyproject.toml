#
[build-system]
requires      = ["setuptools", "setuptools_scm"]
build-backend = "setuptools.build_meta"


[project]
authors = [
    { name = "darkshapes", email = "91800957+exdysa@users.noreply.github.com" },
]
description = "Neural Network Link Library : A comprehensive modular toolkit for Diffusion and Large Language Model inference processes."
dynamic = ["version"]
name = "nnll"
readme = "README.md"
requires-python = ">= 3.11"
keywords = [
    "AI",
    "neural network",
    "library",
    "Diffusion",
    "LLM",
    "identification",
    "URI",
]

classifiers = [
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Intended Audience :: Developers",
    "Intended Audience :: Information Technology",
    "Intended Audience :: Science/Research",
    "License :: OSI Approved :: Mozilla Public License 2.0 (MPL 2.0)",
    "Operating System :: OS Independent",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
    "Topic :: Scientific/Engineering :: Image Processing",
]
dependencies = [
    "accelerate>=1.8.1",
    "aiofiles>=24.1.0",
    "aiohttp>=3.12.13",
    "bitsandbytes>=0.46.0 ; sys_platform != 'darwin'",
    "diffusers  @ git+https://github.com/huggingface/diffusers",
    "gguf>=0.17.1",
    "huggingface-hub[cli,hf-transfer,hf-xet]>=0.33.0",
    "llama-cpp-python>=0.3.9",
    "nnll[cu126,rocm,cpu]; sys_platform != 'darwin' and python_version < '3.12'",
    "nnll[cu128,rocm,cpu]; sys_platform != 'darwin' and python_version >= '3.12' ",
    "nnll[mps]; sys_platform == 'darwin'",
    "peft>=0.15.2",
    "pillow>=11.2.1",
    "protobuf>=5.29.5",
    "psutil>=7.0.0",
    "pydantic>=2.11.7",
    "rich>=14.0.0",
    "sentencepiece ; python_version < '3.13' ",
    "sentencepiece @ git+https://github.com/google/sentencepiece.git#subdirectory=python ; python_version >= '3.13' ",
    "structlog>=25.4.0",
    "transformers  @ git+https://github.com/huggingface/transformers",
]

[dependency-groups]
dev = [
    "nnll[full]",
    "aioresponses>=0.7.8",
    "pytest>=8.4.1",
    "pytest-asyncio>=1.0.0",
    "pytest-mock>=3.14.1",
    "pytest-tornasync>=0.6.0.post2",
    "pytest-trio>=0.8.0",
    "ruff>=0.12.0",
]

[project.optional-dependencies]
cpu = ["torch", "torchvision", "torchaudio"]

mps = [
    "torch",
    "torchvision",
    "torchaudio",
    "mlx-vlm<=0.1.26",
    "mlx-lm<=0.24.1",
    "numpy<=2.2",
    "nnll[openai]",
    "mflux>=0.2.1",
]
cu126 = ["torch", "torchvision", "torchaudio", "nnll[triton]"]
cu128 = ["torch", "torchvision", "torchaudio", "nnll[triton,attention]"]

rocm = [
    "torch",
    "torchvision",
    "torchaudio",
    "pytorch-triton-rocm",
    "nnll[attention]",
]
attention = [
    "sageattention; sys_platform != 'darwin'",
    "flash_attn ; sys_platform == 'Linux'",
]
triton = [
    "triton ; sys_platform == 'Linux'",
    "triton-windows ; sys_platform == 'win32'",
]

lmstudio = ["lmstudio>=1.3.1"]
ollama = ["ollama>=0.5.1"]
openai = [
    "openai>=1.84.0",
    "openai-whisper @ git+https://github.com/openai/whisper.git ; python_version >'3.12'",
    "openai-whisper @ git+https://github.com/openai/whisper.git ; python_version <='3.12'",
    "llvmlite>=0.44 ; python_version > '3.12'",
    "llvmlite>=0.36 ; python_version <= '3.12'",
    "numba>=0.61.2 ; python_version > '3.12'",
    "numba>=0.47.0 ; python_version <= '3.12'",
]
outetts = ["nnll[openai]", "outetts ; sys_platform != 'darwin'"]
# mlx-audio = [
#     "nnll[openai]",
#     "pip>=25.0.1",
#     "mlx-audio @ git+https://github.com/Blaizzy/mlx-audio",
#     "misaki[en] @ git+https://github.com/hexgrad/misaki",
# ]
hidiffusion = ["hidiffusion>=0.1.10"]

full = ["nnll[lmstudio,ollama,outetts,hidiffusion,cu126,cu128,mps]"]

dev = [
    "nnll[full]",
    "aioresponses>=0.7.8",
    "pytest>=8.4.1",
    "pytest-asyncio>=1.0.0",
    "pytest-mock>=3.14.1",
    "pytest-tornasync>=0.6.0.post2",
    "pytest-trio>=0.8.0",
    "ruff>=0.12.0",
]

# vllm = ["vllm>=0.9.0.1 ; sys_platform != 'darwin'"]

# audiocraft = ["audiocraft @ git+https://github.com/exdysa/facebookresearch-audiocraft-revamp.git@x/dev"]
# bagel = ["bagel @ git+https://github.com/exdysa/ByteDance-Bagel.git"]
# orpheus_tts = ["orpheus_tts @ git+https://github.com/canopyai/Orpheus-TTS.git"]
# parler_tts = ["parler-tts @ git+https://github.com/huggingface/parler-tts.git"]
# xllmx = ["xllmx @ git+https://github.com/Alpha-VLLM/Lumina-mGPT"]
# tts = [ "nnll[audiocraft, bagel, parler_tts]", "nnll[audiocraft, bagel, parler_tts,vllm] ; platform_system != 'darwin'",]

[project.urls]
Homepage      = "https://github.com/darkshapes/nnll"
Documentation = "https://github.com/darkshapes/sdbx/wiki"

[project.scripts]
mir-add       = "nnll.mir.mir:main"
mir-maid      = "nnll.mir.maid:main"
nnll-autocard = "nnll.metadata.autocard:main"
nnll-find     = "nnll.compare_layers:main"
nnll-hash     = "nnll.hash256:main"
nnll-inspect  = "nnll.metadata.model_tags:main"
nnll-parse    = "nnll.read_state_dict:main"


[tool.uv]
preview = true
prerelease = "allow"
conflicts = [
    [
        { extra = "cpu" },
        { extra = "mps" },
        { extra = "cu126" },
        { extra = "cu128" },
        { extra = "rocm" },
        { extra = "attention" },

    ],
]

[tool.uv.pip] #ignore: even-better-toml
torch-backend = "auto"

[[tool.uv.index]]
name     = "pytorch-cpu"
url      = "https://download.pytorch.org/whl/cpu"
explicit = true

[[tool.uv.index]]
name     = "pytorch-nightly"
url      = "https://download.pytorch.org/whl/nightly/cpu"
explicit = true


[[tool.uv.index]]
name     = "pytorch-cu126"
url      = "https://download.pytorch.org/whl/cu126"
explicit = true

[[tool.uv.index]]
name     = "pytorch-cu128"
url      = "https://download.pytorch.org/whl/cu128"
explicit = true

[[tool.uv.index]]
name     = "pytorch-rocm"
url      = "https://download.pytorch.org/whl/rocm6.3"
explicit = true

torch = [
    { default-index = "pytorch-cpu", extra = "cpu", marker = "sys_platform != 'darwin'" },
    { default-index = "pytorch-nightly", extra = "mps", marker = "sys_platform == 'darwin'" },
    { default-index = "pytorch-cu126", extra = "cu126", marker = "python_version < '3.12' and sys_platform == 'Linux' or sys_platform == 'win32'" },
    { default-index = "pytorch-cu128", extra = "cu128", marker = "sys_platform == 'Linux' or sys_platform == 'win32'" },
    { default-index = "pytorch-rocm", extra = "rocm", marker = "sys_platform == 'Linux'" },
]
torchvision = [
    { default-index = "pytorch-cpu", extra = "cpu", marker = "sys_platform != 'darwin'" },
    { default-index = "pytorch-nightly", extra = "mps", marker = "sys_platform == 'darwin'" },
    { default-index = "pytorch-cu126", extra = "cu126", marker = "python_version < '3.12' and sys_platform == 'Linux' or sys_platform == 'win32'" },
    { default-index = "pytorch-cu128", extra = "cu128", marker = "sys_platform == 'Linux' or sys_platform == 'win32'" },
    { default-index = "pytorch-rocm", extra = "rocm", marker = "sys_platform == 'Linux'" },
]
torchaudio = [
    { default-index = "pytorch-cpu", extra = "cpu", marker = "sys_platform != 'darwin'" },
    { default-index = "pytorch-nightly", extra = "mps", marker = "sys_platform == 'darwin'" },
    { default-index = "pytorch-cu126", extra = "cu126", marker = "python_version < '3.12' and sys_platform == 'Linux' or sys_platform == 'win32'" },
    { default-index = "pytorch-cu128", extra = "cu128", marker = "sys_platform == 'Linux' or sys_platform == 'win32'" },
    { default-index = "pytorch-rocm", extra = "rocm", marker = "sys_platform == 'Linux'" },
]

[tool.uv.sources]
diffusers     = { git = "https://github.com/huggingface/diffusers" }
sentencepiece = { git = "https://github.com/google/sentencepiece.git", subdirectory = "python", marker = "python_version >= '3.13'" }
transformers  = { git = "https://github.com/huggingface/transformers" }

[tool.setuptools_scm]
write_to = "_version.py"

[tool.setuptools.packages.find]
where   = ["."]
include = ["nnll*"]
exclude = ["tests*"]

[tool.setuptools.package-data]
nnll = ['nnll/integrity/*.json', "nnll/mir/config/*.json"]

[tool.ruff]
line-length    = 140
include        = ["*.py"]
extend-exclude = ["^tests/.*$", "test.*$"]

[tool.pylint]
max-line-length = 140
ignore-paths    = ["^tests/.*$", "test_.*$"]
disable         = ["C0415"]

[tool.ruff.lint.pycodestyle]
max-line-length               = 140
ignore-overlong-task-comments = true

[tool.typos]
files.extend-exclude = ["^tests/.*$", "test.*$"]
default.extend-words = { "ot" = "ot" }
