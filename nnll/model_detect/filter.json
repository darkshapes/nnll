{
    "modelspec": {
        "sai": {
            "blocks": [
                "clip_l",
                "clip_g",
                "t5xxl",
                "mt5",
                "vae.",
                ".mid.",
                ".txt_",
                "unet_down",
                "input_blocks.",
                "mlp.fc1",
                "diffusion_model",
                "down_blocks"
            ]
        },
        "diffusers": {
            "blocks": [
                "0.resnets.0",
                ".attn.to_k.",
                "attentions.1.",
                "add_q_proj.weight",
                "11.ff.net.2.weight",
                "linear_2.weight",
                "norm_out.linear.weight",
                "resnets"
            ]
        }
    },
    "category": {
        "taesd": {
            "blocks": "decoder.layers",
            "tensors": 134
        },
        "dit": {
            "blocks": [
                "context_embedder.weight",
                ".img",
                "adaln_single",
                "extra_embedder",
                "layers.21.norm1",
                "blocks.26.attn1.to_v.weight"
            ]
        },
        "unet": {
            "blocks": [
                "diffusion_model",
                "model.diffusion_model",
                "clf.1.weight",
                "img_",
                "time_embedding.",
                "ffnet"
            ]
        },
        "vit": {
            "blocks": [
                "embeddings.patch_embedding.weight"
            ]
        },
        "language": {
            "blocks": [
                "SelfAttention",
                "self_attention",
                "self_attn"
            ]
        },
        "vae": {
            "blocks": [
                "decoder.mid_",
                "decoder.up"
            ]
        },
        "lora": {
            "blocks": [
                "lora"
            ]
        }
    },
    "lora": {
        "model.lora.flash": [
            "base_model.model."
        ],
        "model.lora.hyper:stable-diffusion-3": [
            "21.norm1_context"
        ],
        "model.lora.pcm": [
            "mid_"
        ]
    },
    "taesd": {
        "model.vae.taesd:stable-diffusion": {
            "file_size": [
                9793292
            ]
        },
        "model.vae.taesd:stable-diffusion-xl": {
            "blocks": [
                " decoder.layers"
            ],
            "file_size": [
                9793292
            ]
        },
        "model.vae.taesd:flux-1": {
            "file_size": [
                9848636
            ]
        },
        "tensors": 134,
        "model.vae.taesd:stable-diffusion-3": {
            "file_size": [
                9848636
            ]
        }
    },
    "dit": {
        "model.dit.auraflow": {
            "blocks": [
                "mlpX.c_fc2.weight",
                "joint_transformer_blocks.2.ff_context.linear_2.weight"
            ],
            "shapes": [
                8192,
                3072
            ]
        },
        "model.dit.hunyuan": {
            "blocks": [
                "extra_embedder",
                "model.blocks",
                "skip_norm.weight"
            ]
        },
        "model.dit.lumina-next": {
            "blocks": [
                "time_caption",
                "feed_forward"
            ]
        },
        "model.dit.pixart-sigma": {
            "blocks": [
                "adaln_single",
                "scale_shift_table"
            ]
        },
        "model.dit.pixart-alpha": {
            "blocks": [
                "aspect_ratio",
                "y_embedding",
                "emb.resolution",
                "caption_projection"
            ]
        },
        "model.dit.lumina-mgpt": {
            "blocks": [
                "model.embed_tokens.weight"
            ]
        },
        "model.dit.flux-1": {
            "blocks": [
                "double_blocks.12.txt_mod.lin.weight",
                "add_q_proj.weight",
                "single_transformer_blocks.9.norm.linear.weight"
            ]
        },
        "model.dit.stable-diffusion-3": {
            "blocks": [
                "model.diffusion_model.joint_blocks.",
                "transformer_blocks.21.norm1_context.linear.weight",
                "transformer_blocks.31.norm1_context.linear.weight",
                "blocks.11.ff.net.2.weight"
            ]
        }
    },
    "unet": {
        "model.unet.stable-diffusion-1:common-canvas-s": {
            "blocks": [
                "up_blocks.3.attentions.0.transformer_blocks.0.norm3.weight"
            ]
        },
        "model.unet.stable_cascade:stage-c": {
            "blocks": [
                "down_blocks.0.2.kv_mapper",
                "previewer",
                "backbone"
            ]
        },
        "model.unet.stable_cascade:stage-b": {
            "blocks": [
                "0.2.channelwise",
                "clip_mapper.bias",
                ".12.self_attn.k_proj.weight"
            ]
        },
        "model.unet.stable_diffusion-1": {
            "blocks": [
                "model.diffusion_model.output_blocks.9"
            ]
        },
        "model.unet.stable-diffusion-xl:refiner": {
            "blocks": [
                "r'conditioner.embedders.0.model.transformer.resblocks.d+.mlp.c_proj.bias'"
            ]
        },
        "model.unet.kolors": {
            "blocks": [
                ".DenseReluDense.wi.weight",
                "encoder_hid_proj.weight"
            ]
        },
        "model.unet.stable-diffusion-xl:playground-25": {
            "blocks": [
                "edm_mean"
            ],
            "shapes": [
                1,
                4,
                1,
                1
            ],
            "tensors": [
                2516
            ]
        },
        "model.unet.stable-diffusion-xl:base": {
            "blocks": [
                "logit_scale",
                "conditioner.embedders.0.transformer.text_model.encoder.layers.0.self_attn.k_proj.weight",
                "add_embedding.linear_2.bias"
            ]
        }
    },
    "language": {
        "model.transformer.bert": {
            "blocks": [
                "pooler.dense.bias"
            ]
        },
        "model.transformer.chatglm": {
            "blocks": [
                ".glm"
            ]
        },
        "model.transformer.t5:base": {
            "blocks": [
                "encoder.block.0.layer.1.DenseReluDense.wi.weight"
            ]
        },
        "model.transformer.umt-5": {
            "blocks": [
                "encoder.block.1.layer.0.SelfAttention.relative_attention_bias.weight"
            ]
        },
        "model.transformer.t5:xxl": {
            "blocks": [
                "encoder.embed_tokens.weight",
                "text_encoders.t5xxl.transformer.shared.weight",
                "t5xxl"
            ],
            "shapes": [
                4096
            ]
        },
        "model.transformer.mt5:xl": {
            "blocks": [
                "text_encoders.mt5xl.transformer.shared.weight"
            ],
            "shapes": [
                250112,
                2048
            ]
        },
        "model.transformer.clip-g": [
            "31.self_attn.k_proj.weight",
            "text_model.encoder.layers.22.mlp.fc1.weight",
            "clip-g"
        ],
        "model.transformer.clip-l": {
            "blocks": [
                "text_model.encoder.layers.0.mlp.fc1.weight",
                "clip-l"
            ]
        }
    },
    "vit": {
        "model.vit.clip-vit-l": [
            "embeddings.patch_embedding.weight"
        ]
    },
    "vae": {
        "model.vae.flux-1": {
            "blocks": [
                "encoder.down.3.block.1.norm1.bias",
                ".nin_shortcut.bias",
                "up_blocks.0.resnets.2.norm2"
            ],
            "tensors": [
                244
            ]
        },
        "model.vae.stable_cascade": {
            "blocks": [
                "out_block.",
                "channelwise.0"
            ],
            "tensors": 122
        },
        "model.vae.stable-diffusion": {
            "blocks": [
                "running_mean"
            ]
        },
        "model.vae.stable-diffusion-xl": {
            "blocks": [
                "encoder.mid_block.resnets.0.conv2.weight"
            ],
            "shapes": [
                512,
                512,
                3,
                3
            ],
            "tensors": 248
        },
        "model.vae.stable-diffusion-3": {
            "blocks": [
                "encoder.mid_block.resnets.0.conv2.weight"
            ],
            "shapes": [
                512,
                512,
                3,
                3
            ],
            "tensors": [
                244
            ]
        },
        "model.vae.kolors": {
            "blocks": [
                "mlp.c_proj"
            ]
        }
    }
}